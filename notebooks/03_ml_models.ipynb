{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libs and setting plotting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T15:56:57.487143Z",
     "start_time": "2021-04-03T15:56:57.480819Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from utils.ml_functions import *\n",
    "from utils.process_data import *\n",
    "\n",
    "mp.style.use('ggplot')\n",
    "mp.rcParams['font.family'] = \"serif\"\n",
    "mp.rcParams['font.size'] = 20\n",
    "sns.set(style=\"darkgrid\", font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(\"../data/raw/train.parquet\")\n",
    "df_test = pd.read_parquet(\"../data/raw/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing types of train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Cleansy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following the directions given in the clarification email sent by topminds stone (it is available in body text of the file `clarification_email.txt`), the predictions must be made in the 90th day after the loan. So, to avoid target leakage, all rows with information about the days after the 90th day are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T15:57:33.142000Z",
     "start_time": "2021-04-03T15:57:13.021965Z"
    },
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def data_cleanse(df):\n",
    "    #deleting rows in the raw data\n",
    "    df.drop(index=df.index[df['dias_pos_desembolso'] > 89], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True) \n",
    "    return df\n",
    "\n",
    "df_train = data_cleanse(df_train)\n",
    "df_test = data_cleanse(df_test)  \n",
    "    \n",
    "# #deleting rows in the raw data\n",
    "# df.drop(index=df.index[df['dias_pos_desembolso'] > 89], inplace=True)\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# #deleting rows in dict_loans_timeseries_attributes\n",
    "# for index, tmp_df_timeseries in dict_dfs_timeseries.items():\n",
    "#     tmp_df_timeseries.drop(index=tmp_df_timeseries.index[tmp_df_timeseries['dias_pos_desembolso'] > 89], inplace=True)\n",
    "#     tmp_df_timeseries.reset_index(drop=True, inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting proper types\n",
    "dtypes = {\n",
    "    'id': int,\n",
    "    'dias_pos_desembolso': int,\n",
    "}\n",
    "\n",
    "df_test = df_test.astype(dtypes) \n",
    "\n",
    "df_train = df_train.astype(dtypes) \n",
    "df_train['y'] = df_train['y'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T15:57:12.983408Z",
     "start_time": "2021-04-03T15:56:58.307344Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extracting a daframe with just the constant attributes\n",
    "def  from_sorted_df_get_constant_attributes(df, constant_attributes, id_column_name=\"id\"\n",
    "):\n",
    "\n",
    "    unique_index = df[id_column_name].unique()\n",
    "    unique_index.sort()\n",
    "\n",
    "    df_constant_attributes = pd.DataFrame(\n",
    "        columns=constant_attributes, index=unique_index.astype(int)\n",
    "    )\n",
    "\n",
    "    company_ids_in_df = df[id_column_name].to_numpy()\n",
    "\n",
    "    for idx in unique_index:\n",
    "        idx_boolean_list = company_ids_in_df == idx\n",
    "        tmp_df = df.loc[idx_boolean_list]\n",
    "        df_constant_attributes.loc[idx] = tmp_df[constant_attributes].iloc[-1]\n",
    "\n",
    "    return df_constant_attributes\n",
    "\n",
    "\n",
    "constant_attributes = ['desembolso', 'vencimento', 'valor_emprestado', 'pgto_diario_esperado', 'subsegmento', 'y']\n",
    "\n",
    "df_train_constant = from_sorted_df_get_constant_attributes(df_train, constant_attributes)\n",
    "df_test_constant = from_sorted_df_get_constant_attributes(df_test, constant_attributes[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_constant.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From datetime to days\n",
    "def from_date_to_days(df):\n",
    "    df[\"desembolso\"] = pd.to_datetime(\n",
    "        df[\"desembolso\"], format=\"%Y-%m-%d\")\n",
    "    df[\"vencimento\"] = pd.to_datetime(\n",
    "        df[\"vencimento\"], format=\"%Y-%m-%d\")\n",
    "    \n",
    "    df['duracao_esperada'] = (df.vencimento - df.desembolso).dt.days\n",
    "    \n",
    "    df.drop(columns=['desembolso', 'vencimento'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_train_constant = from_date_to_days(df_train_constant)\n",
    "df_test_constant = from_date_to_days(df_test_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_constant.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['desembolso', 'vencimento', 'valor_emprestado', 'pgto_diario_esperado', 'dia']\n",
    "\n",
    "df_train.drop(columns = drop_columns, inplace=True)\n",
    "df_test.drop(columns = drop_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following steps of Data Normalization will use the Timeseries attributes ( `divida_total`, `divida_principal`, `pagamento_diario`, `amortizacao_principal_diario`, `transacionado` ). Thus to enforce some comparative bias between these value, they will be normalized according to `valor_emprestado`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_normalization(df, df_constant):\n",
    "    # Getting the numpy array to improve performance\n",
    "    timeseries_attributes = ['divida_total', 'divida_principal', 'pagamento_diario', 'amortizacao_principal_diario', 'transacionado']\n",
    "    df_timeseries_array = df[timeseries_attributes].values\n",
    "\n",
    "    #this loop takes approx. 1.5 min to run\n",
    "    for loan_index in df_constant.index:\n",
    "        loan_value = df_constant.loc[loan_index, 'valor_emprestado']\n",
    "        loan_index_in_array = df.index[df['id'].isin([loan_index])]\n",
    "\n",
    "        df_timeseries_array[loan_index_in_array, :] /= loan_value\n",
    "\n",
    "    df[timeseries_attributes] = df_timeseries_array\n",
    "    \n",
    "    return df\n",
    "    \n",
    "df_train_norm = data_normalization(df_train, df_train_constant)\n",
    "df_test_norm = data_normalization(df_test, df_test_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating Attrubutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here I am creating 8 variables by aggregating the timeseries attributes into a single dataframe:\n",
    "#### `pagamento_diario_total` is the summation of the daily registered payment\n",
    "#### `amortizacao_diario_total` is the summation of the daily registered amortization\n",
    "#### `transacao_diaria_total` is the summation of the daily registered transactions\n",
    "#### `divida_total_menos_principal_area` is the approximated area of curve made by the data points of divida_total - divida_principal\n",
    "#### `divida_total_variacao`  is the inital loan value - the value at the day of prediction\n",
    "#### `angulo_esperado_decaimento_divida` is the angle of the line that fits the expected debt variation\n",
    "#### `angulo_fitado_decaimento_divida`is the angle of the line that fits the main debt variation (counted from y axis)\n",
    "#### `score_do_fit`  is the score of the fit of the line that fits the main debt variation (counted from y axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "new_attributes = ['pagamento_diario_total',\n",
    "                  'amortizacao_diario_total',\n",
    "                  'transacao_diaria_total',\n",
    "                  'divida_total_menos_principal_area',\n",
    "                  'divida_total_variacao',\n",
    "                  'angulo_esperado_decaimento_divida',\n",
    "                  'angulo_fittado_decaimento_divida',\n",
    "                  'score_do_fit']\n",
    "\n",
    "def aggregate_timeseries(df_total, df_constant):\n",
    "\n",
    "    timeseries_attributes = ['divida_total', 'divida_principal',\n",
    "        'pagamento_diario', 'amortizacao_principal_diario', 'transacionado']\n",
    "\n",
    "    df = df_total.copy()\n",
    "    df_constant_new = df_constant.copy()\n",
    "    \n",
    "    df_values = df[['id'] + timeseries_attributes].values\n",
    "\n",
    "    linear_regression = LinearRegression()\n",
    "\n",
    "    df_constant_new[new_attributes] = 0\n",
    "\n",
    "    for loan_index in df_constant_new.index:\n",
    "        tmp_loan_constant = df_constant_new.loc[loan_index]\n",
    "        df_values_index = df_values[:,0] == loan_index\n",
    "        tmp_loan_timeseries = df_values[df_values_index, 1:]\n",
    "\n",
    "        tmp_summed_daily_payment = sum(tmp_loan_timeseries[:, 2])\n",
    "        tmp_summed_daily_amortization = sum(tmp_loan_timeseries[:, 3])\n",
    "        tmp_summed_daily_transaction = sum(tmp_loan_timeseries[:, 4])\n",
    "\n",
    "        tmp_total_minus_main_debt_area = np.trapz(\n",
    "            tmp_loan_timeseries[:, 0]-tmp_loan_timeseries[:, 1])\n",
    "\n",
    "        # if positive the debt decrease\n",
    "        tmp_total_debt_variation = tmp_loan_timeseries[0, 0] - tmp_loan_timeseries[-1, 0]\n",
    "\n",
    "        tmp_expected_slope_of_debt_payment = np.arctan(\n",
    "            (tmp_loan_timeseries[:, 1].max() * 100) / tmp_loan_constant['duracao_esperada'])\n",
    "        tmp_expected_slope_of_debt_payment += np.pi / 2\n",
    "\n",
    "        y = tmp_loan_timeseries[:, 0] * 100\n",
    "        X = np.arange(0, y.size).reshape(y.size, 1)\n",
    "        reg = linear_regression.fit(X, y)\n",
    "\n",
    "        tmp_tg_theta = (reg.intercept_ - (reg.intercept_ +\n",
    "                    reg.coef_[0]*X.flatten().max())) / (X.flatten().max())\n",
    "\n",
    "        tmp_fitted_slope_of_debt_payment = np.arctan(tmp_tg_theta)\n",
    "        tmp_fitted_slope_of_debt_payment += np.pi / 2\n",
    "\n",
    "        tmp_linear_regression_score = reg.score(X, y)\n",
    "\n",
    "        df_constant_new.loc[loan_index, new_attributes] =[tmp_summed_daily_payment,\n",
    "                                                          tmp_summed_daily_amortization,\n",
    "                                                          tmp_summed_daily_transaction,\n",
    "                                                          tmp_total_minus_main_debt_area,\n",
    "                                                          tmp_total_debt_variation,\n",
    "                                                          tmp_expected_slope_of_debt_payment,\n",
    "                                                          tmp_fitted_slope_of_debt_payment,\n",
    "                                                          tmp_linear_regression_score]\n",
    "\n",
    "\n",
    "    return df_constant_new\n",
    "\n",
    "\n",
    "df_train_new = aggregate_timeseries(df_train_norm, df_train_constant)\n",
    "df_test_new = aggregate_timeseries(df_test_norm, df_test_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train_new.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=4, ncols=2, figsize=(16,18))\n",
    "ax = ax.flatten()\n",
    "\n",
    "new_attributes = ['pagamento_diario_total',\n",
    "                  'amortizacao_diario_total',\n",
    "                  'transacao_diaria_total',\n",
    "                  'divida_total_menos_principal_area',\n",
    "                  'divida_total_variacao',\n",
    "                  'angulo_esperado_decaimento_divida',\n",
    "                  'angulo_fittado_decaimento_divida',\n",
    "                  'score_do_fit']\n",
    "\n",
    "for i, attribute in enumerate(new_attributes):\n",
    "    sns.histplot(ax=ax[i], data=df_train_new, x=attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the graphs above, clearly some of the new attributes have a boolean behaviour, other have a bin type characteristic, and others are too much skewed. They will be transformed accordinly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Mapping specifc subsegments into broader comercial sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T15:57:33.190316Z",
     "start_time": "2021-04-03T15:57:33.163206Z"
    }
   },
   "outputs": [],
   "source": [
    "def subsegments_to_segment(df):\n",
    "    sectors = {\n",
    "        'Alimentacao e Bebida': ['Comércio de Alimentos', 'Alimentação Rápida', 'Bares e Restaurantes', 'Comércio de Bebidas', 'Supermercados'],\n",
    "        'Moda e Esports': ['Vestuário', 'Calçados', 'Artigos Esportivos', 'Acessórios, Bolsas e Bijuterias'],\n",
    "        'Industria, Construcao e Veiculos': ['Materiais de Construção', 'Autopeças e Acessórios', 'Comércio de Veículos', 'Gás GLP, Lubrificantes e Outros', 'Reformas e Obras em Geral', 'Postos de Gasolina', 'Estacionamentos e Lava-rápidos', 'Equipamentos de Uso Comercial e Industrial', 'Locação de Veículos'],\n",
    "        'Servicos': ['Oficinas Automotivas', 'Salão de Beleza', 'Conserto de Produtos e Reparos de Peças', 'Outros Serviços - Outros', 'Delivery e Entrega', 'Telecomunicações', 'Academias e Clubes', 'Clinicas de Estética e Massagem', 'Associação', 'Cias Aéreas', 'Jornais e Revistas - Conteúdo Físico', 'Serviços Corporativos - Outros', 'Hotéis / Resorts / Pousadas / Motéis', 'Festas e Eventos', 'Gráfica, Impressão e Xerox', 'Entretenimento e Turismo', 'Consultorias', 'Logística e Mobilidade - Outros', 'Marketing', 'Serviços Imobiliários', 'Segurança', 'Táxi/Carona', 'Paisagismo e Jardinagem', 'Serviços Financeiros', 'Casa e Decoração - Outros'],\n",
    "        'Saude': ['Óticas e Óculos', 'Drogarias e Farmácias', 'Outros Serviços de Saúde', 'Odontologia', 'Veterinários', 'Médicina', 'Outros Produtos de Saúde e Beleza', 'Hospitais e Laboratórios'],\n",
    "        'Comercio': ['Móveis', 'Outros Comércios - Outros', 'Eletrodomésticos', 'Armarinhos e Tecido', 'Tabacaria', 'Cama, Mesa e Banho', 'Cosméticos e Perfumaria', 'Loja de Presentes','Lojas de Departamento', 'Jogos e Brinquedos Físicos', 'Joalherias, Relojoarias e Pratarias', 'Floricultura', 'Petshops', 'Artigos Religiosos e Antiguidades', 'Artigos de Decoração', 'Instrumentos Musicais, CDs, DVDs e Outros'],\n",
    "        'Educacao': ['Extracurriculares, Autoescola e Outros', 'Ensino Básico', 'Livrarias e Papelarias', 'Ensino Superior e Técnico'],\n",
    "        'Informatica': ['Eletrônicos', 'Softwares e Eletrônica Integrada']\n",
    "    }\n",
    "\n",
    "    # checking if all subsegments have been considered. The code below should only generate an N / A text output.\n",
    "    flat_list = [item for sublist in list(sectors.values()) for item in sublist]\n",
    "    for segment in df['subsegmento'].unique():\n",
    "        if not flat_list.count(segment):\n",
    "            print(segment)\n",
    "\n",
    "    # function `return_key_if_contains_value` in process_data.py archive\n",
    "    df['segmento'] = df['subsegmento'].map(lambda segment: return_key_if_contains_value(sectors, segment))\n",
    "    df['segmento'] = df['segmento'].fillna('N/A')\n",
    "    df['segmento'] = df['segmento'].astype('category')\n",
    "\n",
    "    df.drop(columns=['subsegmento'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_train_transformed = subsegments_to_segment(df_train_new)\n",
    "df_test_transformed = subsegments_to_segment(df_test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Binning pagamento_diario_total,  amortizacao_diario_total, transacao_diaria_total, divida_total_variacao, angulo_esperado_decaimento_divida, score_do_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binned_columns = ['pagamento_diario_total', 'amortizacao_diario_total',\n",
    "                     'transacao_diaria_total', 'divida_total_variacao', 'angulo_esperado_decaimento_divida', 'score_do_fit']\n",
    "\n",
    "def binning_features(df):\n",
    "\n",
    "    df_constant_new = df.copy()\n",
    "    df_constant_new['pagamento_diario_total'] = pd.qcut(df_constant_new['pagamento_diario_total'], 2, labels=[\n",
    "                                              'pagamento_diario_alto', 'pagamento_diario_baixo'])\n",
    "    df_constant_new['amortizacao_diario_total'] = pd.qcut(df_constant_new['amortizacao_diario_total'], 2, labels=[\n",
    "                                                'amortizacao_diario_alto', 'amortizacao_diario_baixo'])\n",
    "    df_constant_new['transacao_diaria_total'] = pd.qcut(df_constant_new['transacao_diaria_total'], 2, labels=[\n",
    "                                              'transacao_diaria_alto', 'transacao_diaria_baixo'])\n",
    "    df_constant_new['divida_total_variacao'] = pd.qcut(df_constant_new['divida_total_variacao'], 2, labels=[\n",
    "                                             'divida_total_alto', 'divida_total_baixo'])\n",
    "    df_constant_new['angulo_esperado_decaimento_divida'] = pd.qcut(df_constant_new['angulo_esperado_decaimento_divida'], 2, labels=[\n",
    "                                                         'angulo_esperado_alto', 'angulo_esperado_baixo'])\n",
    "    df_constant_new['score_do_fit'] = pd.qcut(df_constant_new['score_do_fit'], 2, labels=[\n",
    "                                    'score_do_fit_alto', 'score_do_fit_baixo'])\n",
    "    \n",
    "    return df_constant_new\n",
    "\n",
    "df_train_binned = binning_features(df_train_transformed)\n",
    "df_test_binned = binning_features(df_test_transformed)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=len(binned_columns), ncols=2, figsize=(16, 16))\n",
    "\n",
    "for i in range(len(binned_columns)): \n",
    "    sns.histplot(ax=ax[i,0], data=df_train_transformed,\n",
    "                 x=binned_columns[i], hue='y')\n",
    "    sns.countplot(ax=ax[i,1], data=df_train_binned, x=binned_columns[i], hue='y')\n",
    "    ax[i, 0].get_legend().remove()\n",
    "    \n",
    "fig.tight_layout(pad=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_constant_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Power Transform - divida_total_menos_principal_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "def power_transformers(df):\n",
    "    df_constant_transformed = df.copy()\n",
    "    df_constant_transformed['divida_total_menos_principal_area_yeojohnson'] = PowerTransformer().fit_transform(df_constant_transformed['divida_total_menos_principal_area'].values.reshape(-1,1))\n",
    "    df_constant_transformed['valor_emprestado_yeojohnson'] = PowerTransformer().fit_transform(df_constant_transformed['valor_emprestado'].values.reshape(-1,1))\n",
    "    df_constant_transformed['pgto_diario_esperado_yeojohnson'] = PowerTransformer().fit_transform(df_constant_transformed['pgto_diario_esperado'].values.reshape(-1,1))\n",
    "    df_constant_transformed.drop(columns=['divida_total_menos_principal_area', 'valor_emprestado', 'pgto_diario_esperado'], inplace=True)\n",
    "    return df_constant_transformed\n",
    "\n",
    "df_train_powered = power_transformers(df_train_binned)\n",
    "df_test_powered = power_transformers(df_train_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=1, ncols=2, figsize=(16,4))\n",
    "sns.histplot(ax=ax[0], data=df_train_binned['divida_total_menos_principal_area'])\n",
    "sns.histplot(ax=ax[1], data=df_train_powered['divida_total_menos_principal_area_yeojohnson'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Power Transform - valor_emprestado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=1, ncols=2, figsize=(16,4))\n",
    "sns.histplot(ax=ax[0], data=df_train_binned['valor_emprestado'])\n",
    "sns.histplot(ax=ax[1], data=df_train_powered['valor_emprestado_yeojohnson'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Power Transform - pgto_diario_esperado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=1, ncols=2, figsize=(16,4))\n",
    "sns.histplot(ax=ax[0], data=df_train_binned['pgto_diario_esperado'])\n",
    "sns.histplot(ax=ax[1], data=df_train_powered['pgto_diario_esperado_yeojohnson'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking relation between attributes\n",
    "numerical_cols = [cname for cname in df_train_powered.columns if df_train_powered[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "cov_df_loan = df_train_powered[numerical_cols].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "sns.heatmap(ax=ax, data=cov_df_loan, cmap='coolwarm_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pgto_diario_esperado and valor_emprestado seem to be highly correlated..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encoding Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categorical_cols = [cname for cname in df_train_powered.columns if df_train_powered[cname].dtype.name in ['category']]\n",
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for categorical_col in categorical_cols: \n",
    "    df_train_powered = pd.concat([df_train_powered, pd.get_dummies(df_train_powered[categorical_col])], axis=1)\n",
    "    df_test_powered = pd.concat([df_test_powered, pd.get_dummies(df_test_powered[categorical_col])], axis=1)\n",
    "    \n",
    "    df_train_powered.drop(columns=categorical_col, inplace=True)\n",
    "    df_test_powered.drop(columns=categorical_col, inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_powered.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_powered\n",
    "df_test_powered\n",
    "\n",
    "df_train_scalled = df_train_powered.copy()\n",
    "df_test_scalled = df_test_powered.copy()\n",
    "\n",
    "# StandardScale angulo_fittado_decaimento_divida, divida_total_menos_principal_area_yeojohnson\n",
    "scalling_attributes = ['angulo_fittado_decaimento_divida', 'divida_total_menos_principal_area_yeojohnson']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "df_train_scalled['angulo_fittado_decaimento_divida'] = StandardScaler().fit_transform(df_train_scalled['angulo_fittado_decaimento_divida'].values.reshape(-1,1))\n",
    "df_train_scalled['divida_total_menos_principal_area_yeojohnson'] = StandardScaler().fit_transform(df_train_scalled['divida_total_menos_principal_area_yeojohnson'].values.reshape(-1,1))\n",
    "\n",
    "df_test_scalled['angulo_fittado_decaimento_divida'] = StandardScaler().fit_transform(df_test_scalled['angulo_fittado_decaimento_divida'].values.reshape(-1,1))\n",
    "df_test_scalled['divida_total_menos_principal_area_yeojohnson'] = StandardScaler().fit_transform(df_test_scalled['divida_total_menos_principal_area_yeojohnson'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(18,6))\n",
    "\n",
    "for i, attribute in enumerate(scalling_attributes):\n",
    "    sns.histplot(ax=ax[i,0], data=df_train_scalled[attribute])\n",
    "    sns.histplot(ax=ax[i,1], data=df_train_scalled[attribute])    \n",
    "    \n",
    "fig.tight_layout(pad=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Imputing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_scalled.y = df_train_scalled.y.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dealing with Unbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled = df_train_scalled.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(df_sampled['y'])\n",
    "\n",
    "plt.title(\"Unbalanced Data Set\");\n",
    "\n",
    "n_defaults = sum(df_sampled['y'] == 1)\n",
    "n_not_defaults = sum(df_sampled['y'] == 0)\n",
    "\n",
    "print(f\"{n_defaults} Defaults, {n_not_defaults} not Default.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sampling the original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It Is visible that the data set is unbalance. Some sampling tecniques (Undersampling and oversampling) can be applied to avoid some overffiting is classification models due to an unbalanced dataset. But before applying these tecniques, the original dataset must be preserved for testing. So, the models will be fitted with the a dataset that were exposed to some sampling tecnique and, afterwards, it will be tested with the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "X = df_sampled.drop('y', axis=1)\n",
    "y = df_sampled['y']\n",
    "\n",
    "stratified_fold = StratifiedKFold(n_splits=3, random_state=None, shuffle=False)\n",
    "\n",
    "for train_index, test_index in stratified_fold.split(X, y):\n",
    "    original_X_train, original_X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    original_y_train, original_y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "# Convert to array for the ml model\n",
    "original_X_train = original_X_train.values\n",
    "original_X_test = original_X_test.values\n",
    "original_y_train = original_y_train.values\n",
    "original_y_test = original_y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 UnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shuffling the dataset\n",
    "df_sampled.sample(n=df_sampled.shape[0], random_state=1)\n",
    "\n",
    "default_df = df_sampled.loc[df_sampled['y'] == 1]\n",
    "\n",
    "#number of default loans\n",
    "default_amount = (df_sampled['y'] == 1).sum()\n",
    "\n",
    "# select same number of non defaults\n",
    "non_default_df = df_sampled.loc[df_sampled['y'] == 0][:default_amount]\n",
    "\n",
    "balanced_df = pd.concat([default_df, non_default_df])\n",
    "\n",
    "# Shuffle dataframe rows\n",
    "balanced_df = balanced_df.sample(n=balanced_df.shape[0], random_state=1)\n",
    "\n",
    "balanced_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Ml Models for the UnderSample Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "X = balanced_df.drop('y', axis=1)\n",
    "y = balanced_df['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "log_reg_params = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'max_iter': [500]}\n",
    "\n",
    "log_grid_search = GridSearchCV(\n",
    "    LogisticRegression(), log_reg_params,\n",
    "    refit=True, n_jobs=-1)\n",
    "\n",
    "log_grid_search.fit(X_train, y_train)\n",
    "log_reg = log_grid_search.best_estimator_\n",
    "\n",
    "y_pred = log_grid_search.predict(X_test)\n",
    "confusion_matrix_plot(y_test, y_pred)\n",
    "\n",
    "print(f\"Recall Score: {recall_score(y_test, y_pred)}\")\n",
    "print(f\"Auc Score: {roc_auc_score(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_params = {\n",
    "    \"n_estimators\": [50, 70, 90, 110], \n",
    "    \"max_depth\": [4, 6, 8, 10, 12], \n",
    "    \"max_features\": [0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "rf_grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state = 1), \n",
    "    rf_params, n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "rf_best = rf_grid_search.best_estimator_\n",
    "\n",
    "y_pred = rf_best.predict(X_test)\n",
    "confusion_matrix_plot(y_test, y_pred)\n",
    "\n",
    "print(f\"Best Parameters: {rf_best}\")\n",
    "print()\n",
    "print(f\"Recall Score: {recall_score(y_test, y_pred)}\")\n",
    "print(f\"Auc Score: {roc_auc_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = rf_best.predict(original_X_train)\n",
    "confusion_matrix_plot(original_y_train, y_pred)\n",
    "\n",
    "print(f\"Recall Score: {recall_score(original_y_train, y_pred)}\")\n",
    "print(f\"Auc Score: {roc_auc_score(original_y_train, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING ORIGINAL TEST DATA WITH RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = df_test_scalled.index\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "x_test = imp.fit_transform(df_test_scalled.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_best.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prod = rf_best.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(columns=['id', 'y', 'y_prod'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['id'] = indexes\n",
    "submission['y'] = y_pred\n",
    "submission['y_prod'] = y_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_parquet(\"../submission.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}